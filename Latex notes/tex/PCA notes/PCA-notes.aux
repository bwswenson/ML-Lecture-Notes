\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Introduction}{2}{section.0.1}\protected@file@percent }
\newlabel{S-def1}{{1}{2}{Introduction}{equation.0.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {0.2}PCA: Maximum Variance Perspective}{2}{section.0.2}\protected@file@percent }
\MT@newlabel{S-def1}
\newlabel{maxvar_opt_prob}{{2}{3}{PCA: Maximum Variance Perspective}{equation.0.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2.1}Digression: Lagrange Multipliers}{3}{subsection.0.2.1}\protected@file@percent }
\newlabel{eq:opt1}{{3}{3}{Digression: Lagrange Multipliers}{equation.0.2.3}{}}
\MT@newlabel{eq:opt1}
\MT@newlabel{eq:opt1}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2.2}Return from digression}{4}{subsection.0.2.2}\protected@file@percent }
\MT@newlabel{maxvar_opt_prob}
\MT@newlabel{maxvar_opt_prob}
\MT@newlabel{maxvar_opt_prob}
\MT@newlabel{maxvar_opt_prob}
\MT@newlabel{maxvar_opt_prob}
\newlabel{fig:constrained-eig-opt}{{0.2.2}{5}{Return from digression}{subsection.0.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of constrained optimization problem. The objective is a parabaloid constrained to the unit sphere. The parabaloid should look less symmetrical, but my drawing skills aren't great. Below, the unit sphere is unwrapped and the constrained objective is shown.}}{5}{figure.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.3}PCA: Minimum Reconstruction Loss Perspective}{5}{section.0.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.4}Interpreting PCA}{6}{section.0.4}\protected@file@percent }
\newlabel{eq:PCA-proj}{{4}{6}{Interpreting PCA}{equation.0.4.4}{}}
\newlabel{fig:pca-proj}{{0.4}{6}{Interpreting PCA}{equation.0.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The projection operator $P$ embeds data to a lower dimensional subspace in $\mathbb  {R}^d$}}{6}{figure.0.2}\protected@file@percent }
\MT@newlabel{eq:PCA-proj}
\@writefile{toc}{\contentsline {section}{\numberline {0.5}Examples}{6}{section.0.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.6}Unifying max variance and min reconstruction loss perspectives}{6}{section.0.6}\protected@file@percent }
\newlabel{fig:pca-latent-codes}{{0.4}{7}{Interpreting PCA}{figure.0.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The projection operator $P$ embeds data to a lower dimensional subspace in $\mathbb  {R}^d$}}{7}{figure.0.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.7}PCA and the Singular Value Decomposition}{7}{section.0.7}\protected@file@percent }
\bibstyle{alpha}
\bibdata{sample}
\setcounter{exercises@finaltotalpoints}{0}
\@writefile{toc}{\contentsline {section}{\numberline {0.8}Kernel PCA}{8}{section.0.8}\protected@file@percent }
\gdef \@abspage@last{8}
