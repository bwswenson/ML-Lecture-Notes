\chapter{Other notes/Daily notes}
\section{Gaussian Distribution: Normalizing Constant 5/24/24}
The standard Gaussian distribution is given by $1/\sqrt{2\pi}\int e^{-\frac{1}{2}x^2/2}$. How do we know this is a valid density function? Specifically, how do we know the normalizing constant is 1? Here's a quick proof. 

Before starting, recall the change of variables formula
$$
\int_{G^{-1}(\Omega)} f(x)\dx = \int_\Omega f(G(x))|\det G(x)|dx.
$$
Here, $G$ is an invertible diffeomorphism. Now, what we're essentially looking for is to show that
$$
\int_-\infty^\infty \exp(-\frac{1}{2}x^2)\dx = \sqrt(2\pi). 
$$
The following trick allows us to compute it. Let $I = int_-\infty^\infty \exp(-\frac{1}{2}x^2)\dx$. We will compute $I^2$ instead of $I$. I'm not sure if there is some good intuition for why this is easier. But it works. We have
\begin{align}
I^2 & = \left( int_-\infty^\infty \exp(-\frac{1}{2}x^2)\dx \right)\left( int_-\infty^\infty \exp(-\frac{1}{2}x^2)\dx \right)\\
& = int_-\infty^\infty \exp(-\frac{1}{2}(x^2+ y^2))\dx.
\end{align}
This is amenable to a conversion to polar coordinates. Now, I'm sure we could apply the change of variables formula directly at this point. But with a little bit of cheating, I'm going to note that what I'd ultimately like to do is integrate over $r\geq 0$. I'm going to observe that if we consider polar coordinates, then integrating a ring of fixed radius from $\theta=0$ to $2\pi$, we have a density of $\int_0^{2\pi} e^{-r^2} d\theta = 2\pi r e^{-r^2}$. So, 
$$
I^2 = \int_{0}^\infty r e^{-\frac{1}{2}r^2}\dr.
$$
Now, apply change of variables. With $G(x) = \sqrt(2x)$ we see that
\begin{align}
I^2 & = 2\pi\int_{0}^\infty \sqrt{x} e^{-x} x^{-1/2} \dx\\
& = 2\pi \int_0^\infty e^{-x}\dx.
\end{align}
The last integral we know how to evaluate, and it evaluates to 1, so we see that $I^2 = 2\pi$ or $I = \sqrt{2\pi}$. 

\section{Polar Change of Coordinates}
When you do a polar change of coordinates, you change from $\dx \dy$ to $r\dr d\theta$. I get confused where the $r$ actually comes from. And, honestly, the way change of variables is applies is a little counterintuitive to me. So, I'm jotting it down. 
For completeness, recall the change of variables formula. 
$$
\int_{G^{-1}(\Omega)} f(x)\dx = \int_\Omega f(G(x))|\det DG(x)|dx.
$$
As a running example, we'll consider how to integrate $\int e^{-(x^2 + y^2)}$. To apply change of variables, let $f(G(r,\theta)) = e^{-r^2}$. The conversion from polar to cartesian is given by 
$$
G(r, \theta) = (r\cos \theta, ~r\sin \theta). 
$$
Let $H:= G^{-1}$ so that $f(x,y) = f(G(H(x, y)))$. Here, we have $H(x,y) = (x^2 + y^2, ~ \arctan{2}(y/x))$. So, we see that 
$$
f(x,y) = e^{x^2 + y^2}.
$$
In order to apply change of variables, note that 
$$
D_{(r,\theta)} G(r,\theta) = 
\begin{pmatrix}
\cos\theta & r\sin\theta\\
\sin\theta & r\cos\theta
\end{pmatrix},
$$
so that $|\det D G| = r(\sin^2\theta \cos^2\theta) = r$. Now, applying change of variables, we see that
$$
\int_{\R^2} e^{-(x^2 + y^2)} dL(\R^2) = \int_{\substack{r \geq 0\\ \theta \in [0, 2\pi)}} e^{-r^2} r dL(\R^2),
$$
where I'm trying to use that notation to denote the standard Lebesgue measure over $\R^2$. Note that if we let $\Omega$ be the domain on the right hand side, then $G^{-1}(\Omega)$ gives us the domain on the left hand side. I'm not sure if I'm actually doing that right, but this should be equivalent to
$$
\int_{\R^2} e^{-(x^2 + y^2)} \dx \dy = \int_{\substack{r \geq 0\\ \theta \in [0, 2\pi)}} e^{-r^2} r \dr d\theta.
$$
And, of course, in general, you have
$$
\int_{\R^2} f(x,y) \dx \dy = \int_{\substack{r \geq 0\\ \theta \in [0, 2\pi)}} f(G(r,\theta)) r \dr d\theta,
$$
where we've carefully defined what the functions on each side of that mean. 

\section{Change of Variables}
This section is about, in my opinion, one of the most useful and beautiful results in calculus. 
\begin{theorem}[Change of Variables] \label{thrm:CoV}
Suppose that $\Omega$ is an open subset of $\R^{n}$ and $G:\Omega\to \R^n$ is a $C^1$ diffeomorphism. If $f$ is $L^1$ integrable over $G(\Omega)$, then 
\begin{equation} \label{eq:CoV}
\int_{G(\Omega)} f(x) \dx = \int_{\Omega} f(G(x)) \,\left| \det DG(x) \right| \dx.
\end{equation}
\end{theorem}
Above, $DG(x)$ is the Jacobian determinant of $G$. 

Sometimes, when integrating, it is helpful to do a change of variables in order to massage an integral into a form that is easier to handle. This change of variables is represented by $G(x)$ inside the integral. However, simply modifying the argument to the integral changes the value of the integral. The change of variables formula tells us the necessary correction term inside the integral. This is $\left| \det DG(x)\right| $. We'll build intuition for what this correction term means later on. But first, we'll go over a few examples to illustrate how eminently useful this result is. 

\vspace{1em}
\noindent 
\textbf{Example:} Change of variables is the multivariable generalization of $u$ substitution, familiar from introductory calculus. Anytime you use $u$-substitution to reformulate some tricky integral, you're using the change of variables formula.  

\vspace{1em} \noindent 
\textbf{Example (Function of a RV)}: Suppose that $X \in \R^n$ is a random variable with density function $f_X$. Let $Y\in \R^n$, with $Y = g(X)$ where $g$ is a $C^1$ diffeomorphism. What is the density function of $Y$? This is an important question that arises often in ML applications and elsewhere. Ignoring questions of measurability, note that for a set $A\subset R^n$ 
\begin{align}
\P(Y \in A) = \P(g(X) \in A) = \P(X \in g^{-1}(A)) = \int_{g^{-1}(A)} f_X(x)\dx. 
\end{align}
Let $h =g^{-1}$ to make notation clearer. Applying change of variables we see that
$$
\int_{h(A)} f_X(x)\dx = \int_A f_X(h(y)) |\det Dh(y))\dy. 
$$
where in the second line we somewhat arbitrarily change the variable of integration from $x$ to $y$ to make the connection to the density $f_Y$ clearer.
Hence,
$$
\P(Y\in A) = \int_A f_X(h(y)) |\det Dh(y))|\dy
$$
Since this holds for all $A$ (ignoring measurability considerations), we see that the integrand on the RHS must be the density of $Y$, hence
\begin{equation} \label{eq:pushforwardY} % is this the pushforward?? TODO: Double check. 
f_Y(y) = f_X(h(y)) |\det Dh(y))|.
\end{equation}
Any time you need an analytic expression for a function of a random variable, the change of variables formula is indispensable. 

For a concrete example of this, suppose that $X$ is uniformly distributed on $[0,1]$ and $Y = X^2$. What is the density of $Y$? It's $f_Y(y) = \frac{1}{2}\frac{1}{\sqrt{y}}$. This is shown in Figure \ref{fig:cov-example1}. Also, see demo in Jupyter notebook.
\begin{figure}[h] \label{fig:cov-example1}
\centering
\centering
\includegraphics[scale = .5]{change_of_variables/cov-example1.png}
\caption{Density of $Y = X^2$, $X\sim U([0,1])$}
\end{figure}

\vspace{1em} \noindent 
\textbf{Example (Normalizing Flow)}: Suppose we have a set of points $(z_i)_i $ where each $z_i \sim p$, i.i.d, where $p$ is some density function. Suppose that we would like to model the density $p$. Consider a change of variables-based approach. Let $f$ be some distribution we know how to sample from easily (e.g., normal or uniform density). Suppose we sample $W\sim f$ and then pass it through some parametric function $g_\theta(x)$, which we assume to be a $C^1$ diffeomorphism. We wish the random variable $Z=g_\theta(W)$ to have a density as similar as $p$ as possible. Letting $h_\theta = g_\theta^{-1}$, change of variables tells us that the density function of $Z=g_\theta(W)$ is
\begin{equation} \label{eq:NF-density}
f(z;\theta) = f(h_\theta(z)) |\det D_z h_\theta(z)| 
\end{equation}
We may fit $f(z; \theta)$ to our dataset $(z_i)$ using a maximum likelihood approach. If we assume $(z_i)$ is iid, then the likelihood of the joint sample is
\begin{equation} 
L(\theta) = \prod_i f(z_i; \theta)  = \prod_i f(h_\theta(z_i)) |\det D_z h_\theta(z_i)|,
\end{equation}
and the negative log likelihood is
\begin{equation} \label{eq:NF-objective}
\ell(\theta) = -\sum_i \left[ \log f(h_\theta(z_i))  + \log |\det D_z h_\theta(z_i)| \right].
\end{equation}
Minimizing \eqref{eq:NF-objective} simply maximizes the likelihood of the data under our model. This provides us with a function $h_\theta$ that allows us to draw samples from an approximation to $p$. Once $\ell$ is minimized, if we let $Z = h^{-1}_\theta(X)$ for $X\sim f$, then the density of $Z$ is approximately $p$. Methods exist for obtaining $h^{-1}_\theta$. Also, if we simply wish to evaluate the approximate density of $p$ at test point, then we simply use \eqref{eq:NF-density}. This method of approximating a density is called a normalizing flow, and is a current research area in ML. There is \emph{a lot} we could say about this, but we'll ignore it to stay focused on CoV for now. Maybe save this for another time.

\vspace{1em} \noindent
\textbf{Example (Law of the unconscious statistician)}: Let $X$ be a random variable with density function $f_X$. Let $g$ be some measurable function. What is the expected value of $g(X)$? It is commonly taken to be, by definition,
\begin{equation} \label{eq:LOTUS}
\E[g(X)] = \int g(X) f_X(x)\dx. 
\end{equation}
\emph{However}, this is not obvious. This is so commonly taken to be the definition of $\E[g(X)]$ that it is known as the law of the unconscious statistician. 

The definition of the expectation of a random variable is
$$
\E[Z] := \int x f_Z(z) dz. 
$$
Consider the simple case where $X$ is scalar-valued and $g$ is a $C^1$ diffeomorphism. Applying \eqref{eq:pushforwardY}, we know that the density function for $Y = g(X)$ is
$$
f_Y(y) = f_X(g^{-1}(y)) |(g^{-1}(y)')
$$
and 
$$
\E[g(X)] = \int y f_X(g^{-1}(y)) |(g^{-1}(y)') \dy. 
$$
Now, apply change of variables again with $y = g(x)$. To do this, we just substitute $g(x)$ in for $y$ above and multiply by $g'(x)$ inside the integral. We arrive at
\begin{align}
\E[g(X)] & = \int g(x) f_X(x) \frac{d}{dy} g^{-1}(\underbrace{g(x)}_{=y}) g'(x) \dx\\
& = \int g(x) f_X(x) \dx,
\end{align}
where in the last line, we recognize from the chain rule that $\frac{d}{dy} g^{-1}(g(x)) g'(x)  = \frac{d}{dx} g^{-1}(g(x)) = 1$. 
Hence, \eqref{eq:LOTUS} is correct. But the result isn't obvious. Here, we only showed it in a simple scalar-valued case with invertible and differentiable $g$. It does hold more broadly. A measure-theoretic treatment shows that it holds when $g$ is a measurable function and the random variable $X$ has a finite mean (I think... double check). 

\vspace{1em} \noindent
\textbf{Example (Reparameterization Trick)}: The CoV formula is critical in using the reparameterization trick, which is the foundation of an important class of variational-inference based methods in ML. Save the details for another time, for the sake of brevity. 

\vspace{1em}
\noindent
\textbf{Example (Polar change of coordinates/Gaussian normalization constant)}: 
Consider evaluating the integral 
\begin{equation}\label{eq:gauss-soln-step}
I = \int e^{-\frac{1}{2}(x^2 + y^2)} \dx \dy. 
\end{equation}
TODO: Just added in the 1/2 above. Make consistent through the rest! It should already be consistent after the easter egg. 
(We'll see at the end why this is actually an extremely useful integral to know.) This integral is radially symmetric and will be easier to evaluate in polar coordinates, where $x = r\cos\theta$ and $y=r\sin\theta$. 
%The application of change of variables here is a bit subtle. (Basically, it's easier to go from polar to cartesian, so we force ourselves to work primarily in that direction.) 
%Let $f$ be the integrand above, and let $G$ be a function such that $f(G(r,\theta)) = e^{-r^2}$. 
The conversion from polar to cartesian is given by the function
$$
G(r, \theta) = (r\cos \theta, ~r\sin \theta). 
$$
Letting $f(x,y)$ be the integrand above, we have
$$
f(G(r,\theta)) = \exp( -(r^2\cos^2\theta + r^2 \sin^2 \theta)) = \exp(-r^2).
$$
%Let $H:= G^{-1}$ so that $f(x,y) = f(G(H(x, y)))$. Here, we have $H(x,y) = (x^2 + y^2, ~ \arctan{2}(y/x))$. So, we see that 
%$$
%f(x,y) = e^{x^2 + y^2}.
%$$
In order to apply change of variables, note that 
$$
D_{(r,\theta)} G(r,\theta) = 
\begin{pmatrix}
\cos\theta & -r\sin\theta\\
\sin\theta & r\cos\theta
\end{pmatrix},
$$
so that $|\det D G| = r(\sin^2\theta \cos^2\theta) = r$. Now, applying change of variables, we see that
$$
\int_{\R^2} e^{-(x^2 + y^2)} dL(\R^2) = \int_{\substack{r \geq 0\\ \theta \in [0, 2\pi)}} e^{-r^2} r dL(\R^2),
$$
where I'm trying to use that notation to denote the standard Lebesgue measure over $\R^2$. Note that if we let $\Omega$ be the domain on the right hand side, then $G^{-1}(\Omega)$ gives us the domain on the left hand side. I'm not sure if I'm actually doing that right, but this should be equivalent to
$$
\int_{\R^2} e^{-(x^2 + y^2)} \dx \dy = \int_{\substack{r \geq 0\\ \theta \in [0, 2\pi)}} e^{-r^2} r \dr d\theta.
$$
And, of course, in general, you have
$$
\int_{\R^2} f(x,y) \dx \dy = \int_{\substack{r \geq 0\\ \theta \in [0, 2\pi)}} f(G(r,\theta)) r \dr d\theta,
$$
where we've carefully defined what the functions on each side of that mean. 

As a final easter egg in this example, note that 
\begin{align}
I = \int_{\substack{r \geq 0\\ \theta \in [0, 2\pi)}} e^{-\frac{1}{2}r^2} r \dr d\theta = 2\pi \int \exp(-r^2) r\dr. 
\end{align}
Applying another change of variables where we let $r = G(u) = u^{1/2}$ we get (I might have missed a minus sign below...)
$$
I = 2\pi \int_0^\infty \exp(-\frac{1}{2}u) du = 2\pi. 
$$
So, we have an explicit evaluation of this integral. But, note that
$$
\left(\int \exp(-\frac{1}{2}x^2) \dx\right)^2 = I
$$
You can see this by just expanding the square explicitly, and then combining everything under a double integral to get \eqref{eq:gauss-soln-step}. So, 
$$
\int \exp(-\frac{1}{2}x^2)\dx = \sqrt{2\pi}. 
$$
or, equivalently, 
$$
\frac{1}{\sqrt{2\pi}}\int \exp(-\frac{1}{2}x^2)\dx = 1
$$
This is how we know what the normalizing constant is for a normal random variable. 
End example. 

\vspace{1em}
\noindent
\textbf{Example} A high-dimensional gaussian is concentrated on the surface of a sphere. Change of variables is useful in deriving this. (Will do later.) 

\vspace{1em}
\noindent
\textbf{Bonus Example 1}: How can you sample from the unit 2sphere (embedded in $R^3$)? Consider using polar coordinates, so $\theta\in [0, 2\pi]$ is an azimuth and $\phi\in [0, \pi]$ is an elevation (This is an angle measured from the north pole.) Can you just sample $\theta, \phi$ uniformly from their sets? (Pause and consider.) Answer: No. To see why, consider a change of variables from uniform to polar coordinates. Suppose that $A$ is some subset of the unit sphere. Then under a uniform distribution on the unit sphere, we have
$$
\P(A) = \frac{1}{4\pi}\int_{A} 1\dx\dy\dz = \frac{1}{4\pi}\int_{g^{-1}(A)} \sin(\phi) \,d\phi \,d\theta,
$$
where we have used the fact that the surface are of the unit sphere is $4\pi$ and the fact that the spherical change of coordinates (see Figure \ref{fig:spherical-coords}) so that
\begin{align}
x = r\sin(\phi)\cos(\theta) \\
y = r\sin(\phi)\sin(\theta) \\
z = r\cos(\phi). 
\end{align}
\begin{figure}[h]

     \centering
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{change_of_variables/spherical-coords1.jpeg}
         \caption{}
     \end{subfigure}
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{change_of_variables/spherical-coords2.jpeg}
         \caption{}
         \label{fig:three sin x}
     \end{subfigure}
        \caption{Spherical coordinates. I accidentally left out the $r$ in this figure. So just multiply everything by a radius $r$.}
        \label{fig:spherical-coords}
\end{figure}
The $\sin(\phi)$ comes from computing $\det DJ(\theta, \phi, r) = r^2\sin(\theta)$.\footnote{This actually raises an important subtlety. If we don't fix $r=1$, then we are going from $\R^3\to \R^3$. The Jacobian is invertible. If we do fix $r=1$, then we have 3 cartesian coordinates and 2 polar coordinates. And change of variables can't be applied. We get around this by computing the change of variables for 3 to 3 coordinates, and then fixing $r=1$, and then integrating only with respect to the two dimensional Lebesgue measure w.r.t. the angles. But this feels like it's playing a bit fast and loose.}
The integrand on the right hand side gives us the density for $\phi, \theta$ that corresponds to a uniform distribution on the sphere. In particular, the density to sample uniformly from the unit sphere in spherical coordinates is
$$
f(\theta, \phi) = \frac{1}{4\pi}\sin(\phi)\ones_{[0, 2\pi]\times [0, \pi]}.
$$
How do you sample from this distribution? Here's a trick that works, that I haven't thought through how you'd come up with from scratch. Let $u,v\sim U([0,1]^2)$. Let $\theta = 2\pi u$ and $v = \cos^{-1}(2v-1)$. Letting
$$
h(\theta, \phi) = g^{-1}(\theta, \phi) = 
\begin{pmatrix}
1/2\pi \theta\\
(\cos(\phi) - 1)/2
\end{pmatrix}
$$ and 
applying \eqref{eq:pushforwardY} we immediately get that $f_{\theta, \phi}(\theta, \phi) = \frac{1}{4\pi}\sin(\phi)$. The domain of the density comes from looking at the range $g(u, v)$. 

It should be noted that this is the hard way of sampling from the unit sphere. The easy way is to sample from a gaussian and then normalizing the samples. But this examples builds intuition that I think can generally be helpful. 

\textbf{Bonus Example 2}: 
Suppose you want to uniformly sample from the set of all rotation matrices in three dimensions. How can you do that? A rotation consists of an axis of rotation and an angle of rotation. You can uniformly pick the axis of rotation as above, and then sample an angle of rotation uniformly from $[0, 2\pi]$. Question: How do you sample a random rotation matrix in higher dimensions? Not sure...


\vspace{1em}
\subsubsection{Building Intuition}

The previous examples have demonstrated the clear utility of being able to change variables inside of an integral. The key term in CoV is the correction term $\left| \det DG(x) \right| $. This gives us the exchange rate required to compensate for the introduction of $g(x)$ inside the integrand. To build intuition about this, consider the example $f(x) = \frac{1}{2}x^2$. Suppose, for some made up reason, that we wish to evaluate the integral $\int_{[0,1]} f(x)\dx$, but we only have access to $f(g(x))$, where $g(x) = cx$, $c>1$.\footnote{We keep $c>0$ to make the discussion of ``compression'' consistent later. But the same reasoning holds for any $c\not=0$, just change the terminology to ``dilation'' where necessary.}
%First, note that 
%$$
%\int_{[0,1]} \frac{1}{2} x^2 = 1. 
%$$
%Now, suppose it is easier for some reason to evaluate the integral by substituting in $y = cx$. We wish to evaluate this same integral, but under the change of variables. 
\begin{figure}[h] \label{fig:cov-compression}
\centering
\centering
\includegraphics[scale = .21]{change_of_variables/CoV-compression.jpeg}
\caption{Illustration of ''compression'' effect.}
\end{figure}
The first thing to notice is that if we're going to evaluate the same integral, we've got to make sure the underlying domains match up. This is illustrated in Figure \ref{fig:cov-compression}. The function $g(x) = cx$ has the effect of compressing the domain of integration. The end point for integration inside the function is reached when $x=1/c$.  We make sure the domains match up by taking the preimage of the desired domain under $g$. Equivalently, we may state this as they do in Theorem \ref{thrm:CoV} in terms of $g$ rather than the primage, via 
$$
\int_{g(\Omega)} f(x)\dx = \int_{\Omega} \mbox{stuff}.
$$
Next, because $g$ compresses the domain of integration, if we directly integrate, we will end up with a value that is too small. Again, see Figure \ref{fig:cov-compression}. We compensate for this compression term by understanding the ``compression rate'' and compensating for it when computing the integral. I'm going to call this the exchange rate. The ``exchange rate'' at which it compresses can be understood by looking at the preimage of the canonical volume-1 set $[0,1]$. This tells us the volume of the set that maps to $[0,1]$. The exchange rate is given by the ratio. 
$$
\frac{\mbox{vol}\, [0,1]}{\mbox{vol} \, g^{-1}([0, 1])}.
$$
Evaluating this expression we get a ``compression ratio'' (I'm making that term up) of $c$. Higher compression ratio means more compressive. 
Because the function $g$ is linear, we can also get the exchange rate by just looking at the ratio
$$
\frac{\mbox{vol} \, g([0, 1])}{\mbox{vol}\, [0,1]}.
$$
This tells us the relative size of the set that $[0,1]$ is mapped to, and gives us the same information about the compression rate in the previous expression. Evaluating this expression gives us the same compression ratio. This ratio tells us the effect that including $g$ inside the argument of $f$ has. In particular, it tells us the rate at which infinitessimal segments are compressed inside the integral. We may now compensate for the infinitetessimal compression effect inside the integral by multiplying by the exchange rate
$$
\int_{[0,1]} \frac{1}{2} x^2 \dx = \int_{[0,1/c]} \frac{1}{2} (cx)^2  c\dx = 1
$$
%Note that in the a nonlinear case, we'd have to be careful to multiply by the local exchange rate--here, linearity gives us a global exchange rate. 
%Returning to linear maps, 
This same idea holds more generally if we consider the effect of a bijective linear map $T:\R^n\to\R^n$. The following result is critical. 
\begin{theorem}
Let $T:\R^n\to\R^n$. Then the image of the hypercube $T([0, 1]^n)$ is a parallelipiped. Representing $T$ as a matrix, the (signed) volume of the parallelipiped is given by $\det T$. 
\end{theorem}
In the previous theorem, the sign on the volume tells us about the orientation of the parallelipiped relative to the original hypercube. We won't go into that here. Taking $|\det T|$ gives the unsigned volume. 

\vspace{1em}
Example: Consider the matrix
$$
T = 
\begin{pmatrix}
0 & 1\\
2 & 3
\end{pmatrix}. 
$$
The image of the cube $[0,1]^2$ is shown in Figure \ref{fig:parallelepiped-2d}. See also Jupyter notebook. 
\begin{figure}[h] \label{fig:parallelepiped-2d}
\centering
\includegraphics[scale = .5]{change_of_variables/parallelepiped-2d.png}
\caption{The image of the cube $[0,1]^2$ under $T$.}
\end{figure}

Note that the vertices are given by the image of the vertices of the hypercube. In particular, two of the vertices are precisely the columns of $T$. 

Returning to change of variables with a linear function $T$ inside of $f$, we see that $|\det T|$ gives our exchange rate inside the integral:
$$
\int_A f(x)\dx = \int_{T^{-1}(A)} f(T(x)) |\det T| \dx.
$$
More generally, if the function $g$ is nonlinear, then we must use an instantaneous exchange rate. This is obtained by linearizing $g$ at each point $x$, and computing the volume exchange rate for the linearization; i.e., $|\det Dg(x)|$. 
This result takes a measure theoretic treatment to prove generally. But this is the basic intuition. 

Also, a concrete example with a density function would be nice. So you can see how the map is sort of redistributing the mass in the density function, and how the determinant term helps 

\section{High dimensional spaces}
We often work in high dimensional spaces. Many datasets have 10 or more features. MNIST data has 728 dimensions. A high resolution image has a million dimensions. If you're working with a kernel machine, even if your data has relatively low dimensionality, it's embedded in a high dimensional space. We develop intuition in 2 and 3 dimensions. But our basic mental picture fails in serious ways to generalize in higher dimensions, where properties are counterintuitive, and sometimes quite bizarre at first sight. The purpose of this discussion is to help build intuition about high dimensional spaces so you can be a better data scientist, ML practitioner, or engineer. Also, understanding the basic geometric properties of high dimensional spaces lays the foundation for better understanding statistical properties in high dimensional spaces. 

\subsection{Volume of $d$-ball}
The ball of radius $r$ in $\R^d$ is given by
$$
B_r := \{ x\in \R^d: \|x\| \leq r\}.
$$
The volume of the ball in $d$ dimensions is obtained by integration. There are a few different ways of doing this, some pretty clever.\footnote{I was tempted to put in the version that uses the Gaussian integral, since we solved that in the CoV section. But it makes an assumption about a proportionality relationship with the surface area of high dimensional spheres that isn't obvious. And I didn't want to have to prove. And integrating this stuff out isn't really the point of these notes. So, I took the lazy route of just giving a high level idea of the brute force approach, but skipping the details because they're tedious and don't advance the objective of these notes.} But the brute force way is to just integrate in $d$ dimensions analogous to how you've done it in 3. Set up the integral you want to solve in cartesian coordinates.
$$
\int_{B_r} \dx_1\cdots \dx_d.
$$
Do a change of variables to spherical coordinates. (Hyper)-spherical coordinates in $d$ dimensions are defined analogous to the 3d case, and you can look up the conversion and wrap your head around how it works. At the end of the day, you solve the integral
$$
\int r^{d-1} \sin^{d-2}(\phi_1)\cdots\sin(\phi_{d-1}) d\phi_{d-1}\cdots d\phi_{1} dr,
$$
where I've omitted the domain because I'm lazy. But it's a product of intervals. So, a high dimensional rectangle. You chug through this integral and you get that the volume of the $d$ ball of radius $r$ is given by
\begin{equation} \label{eq:vol-of-ball}
V_d(r) = \frac{\pi^{d/2}r^d}{\Gamma(\frac{d}{2} + 1)}. 
\end{equation}
Just think of the Gamma function as an extension of the factorial to non integer values. 
Figure \ref{fig:volumes-of-balls}, taken from wikipedia, plots the volume of a few different balls as the dimension increases. Note that the volume of a ball plummets to zero as $d\to\infty$. For bigger radii, the volume will be a lot bigger in lower dimensions, but no matter what, the volume of any ball goes to zero as $d\to \infty$. That's freaking weird. 
\begin{figure}[h]
     \centering
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{high_dimensional_space/Volumes_of_unit_balls.png}
         \caption{}
     \end{subfigure}
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{high_dimensional_space/Volumes_of_unit_balls_log.png}
         \caption{}
         \label{fig:three sin x}
     \end{subfigure}
        \caption{Volume of unit ball as a function of the dimension. (a) is the standard plot, taken from Wikipedia (b) plots the log of the volume vs dimension.}
        \label{fig:volumes-of-balls}
\end{figure}


%Good way to approach this: Don't state the title about your intuition is wrong. Just ask to speculate about some properties? I don't know. Maybe start by focusing on 1 ball. Consider what volume does as dimension increases. It's bizarre that it goes to zero. Then consider $r>1$. And note that no matter how big the radius of the ball, eventually it goes to zero. This has to do with Euclidean topology. If you consider other $Lp$ balls, I don't think they do this. 



\subsection{Balls and Cubes}
Consider a ball of radius 1/2 inscribed inside a cube of width 1. e.g., 2d example. The ball always touches the surface of the cube. As $d$ increases, the volume of the cube stays one. But the volume of the sphere inside goes to zero (quickly!). As another example, consider a ball of radius 1 and cube of width 1. Draw in 2d. The cube is contained well within the ball. But when we go to 3 dimensions, the gap closes a bit. The corner of the cube is at $\sqrt{d/2^2} = 1/2\sqrt{d}$. At $d=4$ the corners of the unit cube touch the surface of the ball of radius 1. For $d>5$, the corners poke through the surface. Moreover, there are $2^d$ vertices poking through. 


\subsection{Most of the mass is near the equator}
%\begin{figure} \label{fig:volumes-of-balls}
%\centering
%\includegraphics[scale = .5]{high_dimensional_space/Volumes_of_unit_balls.png}
%\caption{Volumes of $d$ balls.}
%\end{figure}
See section 1.2.3 of the CMU book chapter on geometry of high dimensional spaces. 

Consider a small slice through the ball near some equator. That is, consider the portion of the ball that lies "above" the plane where $x_1 = \epsilon$. One can show that very little of the volume of the sphere lies in this region. This is accomplished by simply integrating (and using a few approximations). Add this if I have time. But I'm not sure it's worth the trouble of working through the integration. the end result is the following lemma. 
\begin{lemma}
For any $c>0$, the volume of the hemisphere above the plane $x_1 = \frac{c}{\sqrt{d-1}}$ is less than $\frac{2}{c}e^{-c^2/2}$. 
\end{lemma}
Intuitively, I think this makes a lot of sense. You're fixing a plane by setting $x_1=0$ and considering a $\epsilon$ padding of that plane, and considering how much mass is in that padded region. While it's true that $\epsilon$ is small, by considering a plane, you're considering an $d-1$ dimensional set. When you move in the orthogonal direction, you only have 1 dimension to move in. In some sense, this dimension can't contain a lot of volume. Most of the volume is contained near the plane. 

You can get some intuition for this by going from 2 dimensions to 3 dimensions. If you fix a diametric plane in 2 dimensions, then moving orthogonal to the plane you capture a lot of volume. Now do the same thing in three dimensions. More of the volume is contained near the equatorial plane. As the dimension increases, more and more of the volume is contained near $d-1$ dimensional equatorial hyperplane. 

While the idea that most of the mass is near the equator, its implication in practice is almost stupidly obvious. Suppose you draw a uniform random sample from the unit ball. Then with high probability, all of the coordinates are going to be small. This makes sense when you think of the fact that the norm of the sample must be contained near within a radius of $1$. If any one of the components is close to 1, then the others have to be really, really tiny. Another way to think about it, suppose that $x_i = $ big, for some coordinate $i$. This means that the other coordinates must all be contained in a very small $d-1$ dimensional ball. But the product of a tiny $d-1$ dimensional ball with, even the entire diameter of the circle, is going to have really small volume. So, that's not going to happen. 

\subsection{Most of the volume is near the surface}
(Copied from cmu book.) By \eqref{eq:vol-of-ball}, the ratio of the volume of a sphere of radius $1-\eps$ to the volume of a unit sphere in $d$ dimensions is $
(1-\eps)^d.$
This is shown in Figure \ref{fig:vol-near-surf}. 
The higher the dimension, the more the volume is concentrated on the surface of the sphere. This basically follows from the fact that the higher $d$, the more $x^d$ looks flat below 1 and like a wall straight up at 1. 
\begin{figure} \label{fig:vol-near-surf}
\centering
\includegraphics[scale = .5]{high_dimensional_space/vol-near-surf.png}
\caption{Volume near surface of sphere.}
\end{figure}

\subsection{High dimensional cubes}
The fact that most of the volume of a high dimensional ball is near the surface is a little more transparent when you look at hypercubes. (Which, I'm going to call a ball, even though by ball here, we really do only mean L2 ball, which is an important distinction. The volume near the surface property is probably true for any convex body?) 
Consider a unit cube, and the volume outside the inner cube with side length $1-\epsilon$. Draw this out for 1, 2, and 3d. The volume of the inner region diminishes as $(1-\eps)^d$. (See other notes on this for a compelling presentation.) 


\subsection{Distances between randomly sampled points}
Suppose you sample two points uniformly at random from the unit ball. How far apart will they be? Show demo. Answer is that even though the volume of the ball is going to zero, the distance between randomly sampled points converges to a constant. State a general result about distance between furthest and closes points, for iid case. (Technically, this won't cover the ball case. But it tells us something.) 

Question: What does this mean about things like knn? Under reasonably general conditions, you can show that as the dimension grows large, the distinction between nearest and furthest point vanishes. An example of this is the following result. 
\begin{theorem}
Suppose that $X_n\in \R^d$ is composed of $d$ iid random variables and that for some $k\geq 1$, $\lim_{d\to\infty} \frac{\|X_d\|_k}{\E \|X_d\|_k} = 0$. Then 
$$
\frac{D_{max, d}^k - D_{min, d}^k}{D_{min, d}^k} \to_{p} 0 \quad as \quad d\to \infty,
$$
where $\to_p$ indicates convergence in probability to the point mass on zero. 
\end{theorem}

Example: Sampling points from the $d$-cube. (TODO: Add computational example. TODO: Is there an interesting two-class classification version of this? Like, can you distinguish between points on opposite sides of a hypercube? Probably best to just look at that paper. TODO: If I don't have time to do this now, then that's OK. Maybe this is good fodder for discussion. When does this tell us about when knn breaks down? How does this impact things like neural networks operating in high dimensional spaces?)  

Moral: You need to be careful about the meaning of distance metrics and similarity in high dimensions. 

\subsection{Digression: Lp balls}
Question: We've implicitly looked at l2 and l-infty balls. You can consider similar properties for other lp balls. Note paper about surprising properties of distances. Add reference. Moral of that paper: Can consider fractional distances, and these tend to be a little more meaningful. 

\subsection{Discussion}
Mention things like, is data really usually high dimensional. No. usually does something like live in a lower dimensional manifold. Touch on importance of dimensionality reduction for being able to say meaningful things about data in something like knn. Touch on PCA and isomap. 

Mention that sometimes you intentionally move up to higher dimensions. Usually for kernel methods. This allows you to do things like linearly separate classes of data. Mention Cover's theorem. 

This also makes me wonder about SVMs in high dimensions (not even kernel SVM). Like, if you've got a two-class data set, and you're trying to find the largest margin linear separating hyperplane, this is ultimately a distance problem in a high dimensional space. I've always taken for granted that my intuition from 3 dimensions implies that there are no sticky issues when doing this. But is that true? 


%Outline:
%- High dimensional balls. Definition. Volume of unit ball? (This is a TODO. Would like a relatively clean derivation of this that I can follow fully.) Surface area? (Maybe just state this, not derive, if we're already deriving volume.) Distance between two points sampled from ball? Maybe just show this with an example. Counterintuitive that the volume is going to zero, but the distance between points stays constant? (Is that true?) Most of the volume is at the equator. Derive. Discuss: In 3d, this property seems counterintuitive when you're visualizing a high dimensional ball as just sort of a higher dimensional version of a 3-ball. But it makes more sense when you realize that it's a totally different object. For a 3 ball, if most of the mass is at the equator, then you would think that if you take a different (say ``orthogonal'') equator, then most of the mass can't be there too. Because it's already in the other equator. The point of this example is to show you that your intuition is broken. You can't just extend your intuition from 3 balls here. n-balls are weird objects. Next property: - Intersection of equators? (Show that two strips around different equators have a lot of mass in common. Not sure how to show this.) Most of the volume is in a shell near the surface. Computation example: Sample points iid uniform from the $n$ ball. What's the distance between them? As $n$ grows large, all points tend to be a constant distant apart (experimentally). Hence, the volume of the $n$ ball goes to zero, but points sampled from the ball all tend to be a constant distant apart. High dimensional space is weird. We'll see shortly that this constant distance property has important implications for ML/data science. 
%- High dimensional cubes. Volume of the 1cube stays constant. Unit cube and 1/2 inscribed ball. Volume of the inscribed ball goes to zero. Unit cube and 1-ball. As dimension increases, the corners of the unit cube poke out of the 1ball, and take up all the volume. Again, the volume of the ball goes to zero. But the volume of the cube stays constant. Maybe note that here you can easily see that most of the volume of the ball is again at the surface of the cube. 
%- Comment on the fact that when you take an epsilon ball about a point in $\R^d$ in proofs, you're actually doing this funky thing. 
%- What does all of this have to say about classification in high dimensions? In particular, what does it have to say about KNN classification? Reference: 
%%https://minds.wisconsin.edu/bitstream/handle/1793/60174/TR1377.pdf?sequence=1&ref=https://githubhelp.com. 
%TODO: Briefly look over this and the surprising results about distance metrics in high dimensions. State a summarized version of their theorem that points all about equidistant in high dimensional space. Relate this to an actual sampling and classification problem, so you can see why knn in high dimensions would be problematic. Don't spend a ton more time on this. 
%- A few other incidental questions and issues to bring up. How often does high dimensional data really arise? I mean, often, yes. But is it genuinely high dimensional, or does it live in a lower dimensional space? This brings up the importance of dimensionality reduction techniques for certain types of data processing. PCA is one method, but it only really works if the data lives in a linear subspace. You have to consider your data assumptions. If your data lives on a smooth manifold, then you can use something like isomap. What are other structures the data might have? clusters? spheres in spheres? Like, what are reasonable data topologies you might expect, and what are good ways to ``reduce'' the dimension in order to do analysis. This isn't always simple. e.g., if the data lives on $S^2$ sphere, then how do you reduce it to 1 or two dimensions? This is a cartographers problem. And no matter what you do, you're going to break some of the topological properties. Something you can do to fully capture the relationships mapping to 2d is to produce 2 different maps that slice the sphere along different lines. Then the manifold distance relationships on both are true, even though they seem to conflict somewhat. Something else that's worth considering is if you want to just construct something like a graph that you'll use for label propagation, then you can do it using similarity in high dimensions, under the assumption that the data lives on a reasonably low dimensional manifold. That will actually preserve connections around things like spheres. 
%- Kernel methods. Sometimes we intentionally lift data to a higher dimensional space. The intrinsic dimension of the data is the same as the original dimension. (Footnote to theorem reference.) But in high dimensions, binary data can become linearly separable. There is also kernel PCA,, which doesn't rely on labels. But I'm not sure I actually understand the value of kernel PCA other than understanding what your kernel machine is doing. It's true that it can do things like make concentric rings look linearly separable. But this isn't really well understood. (Possible open research question?) 
%- Something else to integrate: The n-ball above is the Euclidean ball. The hypercube is the Linfty ball. Are certain Lp norms better for certain purposes? This paper is useful: https://bib.dbvis.de/uploadedFiles/155.pdf


\section{High Dimensional Gaussian }
Random variables in high dimensional spaces can behave in counterintuitive ways. We typically develop intuition in low dimensions, but don't realize which aspects of our intuition fail to extend to high dimensional spaces. A good place to start building better intuition is looking at the properties of high-dimensional Gaussian random variables. The main property of high dimensional Gaussians that you'll hear cited is that most of the mass is concentrated around the surface of a sphere. The reason for this bizarre effect has to do with more basic properties of high dimensional geometry than anything else. A high-dimensional ball has most of its mass concentrated on the surface of a sphere. We'll begin by briefly considering properties of high-dimensional spheres and hypercubes, and then characterize the high dimensional Gaussian. 

\subsection{Radial density of Gaussian}
Suppose $x\in \R^d$ is sampled from an isotropic Gaussian. We would like to understand the density of $r= \sqrt{\sum_{i=1}^d x_i}$. To this end, first note that the random variable $r$ can be viewed as 
$$
r = \sqrt{w} 
$$
where $w$ is a $\chi^2$ distribution with $d$ degrees of freedom having density
$$
f_w(w) = cw^{\frac{d}{2} - 1}e^{-\frac{d}{2}},
$$
and $c$ is a proportionality constant that I'm ignorning.
Applying the change of variables expression \eqref{eq:pushforwardY}, we can compute the density of $r$. We have $h(r) = g^{-1}(r) = r^2$ and $Dh = 2r$. Subbing this in we get
$$
f_r(r) = cr^{d - 2}e^{-\frac{r^2}{2}}r = cr^{d-1}e^{-\frac{r^2}{2}}, 
$$
where we absorb stuff into the constant. 
We would like to characterize where most of the mass of $f_r(r)$ lies. A plot of $f_r(r)$ for various values of $d$ is shown in Figure ??. In general, the plots suggest that $f_r(r)$ is log concave, and most of the mass is concentrated in a band that increases roughly as $\sqrt{d-1}$. Our plan of attack will be 
\begin{enumerate}
 \item Take the log of the density, so we hopefully deal with a concave function
 \item Consider an interval $I$ of width $2d$ about the maximum of the function. We'll try and characterize the amount of mass in this interval. 
 \item Construct a second order approximation of the function. In particular, construct an upper bound on the function and then use this to bound the amount of mass outside of the interval. 
 \item Because we've been cavalier with the proportionality constant, we don't know what the total mass of $f$ is. (It's not a probability density.) We'll come up with a lower bound on the total mass of $f_r$ and use that to estimate the fraction of mass in the interval $I$. 
 \item Continuing with the previous point, let $M_{\bar I}$ be the mass outside the interval $I$ and let $TM$ denote the total mass. We are interested in estimating
 $$
 \mbox{quantity of interest } = \frac{M_{\bar I}}{TM}.
 $$
We'll compute an upper bound on $M_{\bar I}$ and a lower bound on $TM$, to get an upper bound estimate on the quantity of interest. If we can say that QOI is small, then it means that most off the mass lies in the interval $I$. Through all of this, we'll make our estimate depend on $c$, the width parameter of $I$, so that we'll be able to estimate the amount of mass in $I$ as a function of width. 
\end{enumerate}

We now proceed along those lines. 
Disregarding the proportionality constant, let 
$$
g(r) := r^{d - 2}e^{-\frac{r^2}{2}}r = cr^{d-1}e^{-\frac{r^2}{2}} \quad \mbox{ and } \quad f(r) = \log g(r) = (d-1) \log(r) - \frac{r^2}{2}.
$$
Note that 
$$
f'(r) = \frac{d-1}{r} - r \quad \mbox{ and } f''(r) = \frac{-(d-1)}{r^2} - 1 \leq -1.
$$
The second inequality implies that $f(r)$ is concave (and $g(r)$ log concave). Hence, solving for $f'(r) = 0$, we see that $f$ is maximized at $\sqrt{d-1}$. Applying Taylor's theorem about $r=\sqrt{d-1}$ and we have
\begin{align}
\label{eq:gauss-sphere-f-taylor}
f(r) & = f(\sqrt{d-1}) + \underbrace{f'(\sqrt{d-1})}_{=0}(r-\sqrt{d-1}) + \frac{1}{2}f''(\zeta)(r-\sqrt{d-1})^2\\ 
& \leq f(\sqrt{d-1}) - \frac{1}{2}(r-\sqrt{d-1})^2
\end{align}
for some $\zeta$ between $r$ and $\sqrt{d-1}$. In the first line, we have applied a form of Taylor's theorem where we explicitly handle the remainder. (This can be found on Wikipedia, and would be a good topic for future notes. On the board, maybe set this part up by first stating the standard form of Taylor's theorem. Then backtrack and note that this this other convenient form that allows us to explicitly handle the remainder. Though, to be honest, could you just use the standard form?) In the last line, we apply the fact that $f''(\zeta) \leq -1$. 
By the definition of $f$, this gives
\begin{align}
g(r) & = \exp(f(r)) \\
& \leq \exp(f(\sqrt{d-1}) - \frac{1}{2}(r-\sqrt{d-1})^2 )\\
&  = g(\sqrt{d-1})\exp(-\frac{1}{2}(r-\sqrt{d-1})^2 ).
\end{align}
As desired, we have a simple ``quadratic'' estimator on $g$. Let $r_d = \sqrt{d-1}$, because I'm tired of writing this, and for a constant $c>0$ consider the interval 
$$
I = \{r_d - c, r_d+c\}. 
$$
Let's try to bound the mass outside this interval using our quadratic overestimator. 
\begin{align}
\int_{r\not\in I} g(r)\dr & \leq \int_0^{r_d - c} g(\sqrt{d-1})\exp(-\frac{1}{2}(r-\sqrt{d-1})^2 ) \dr + \int _{r_d+c}^{\infty} g(\sqrt{d-1})\exp(-\frac{1}{2}(r-\sqrt{d-1})^2 )\dr\\
& \leq 2g(\sqrt{d-1})\int _{r_d+c}^{\infty} \exp(-\frac{1}{2}(r-\sqrt{d-1})^2 )\dr\\
& = 2g(\sqrt{d-1})\int _{c}^{\infty} \exp(-y^2/2)\dy\\
& \leq 2g(\sqrt{d-1}) \int _{c}^{\infty} \frac{y}{c}\exp(-y^2/2)\dy\\
& = \frac{2}{c}g(\sqrt{d-1})\exp(-c^2/2).
\end{align}
In the first line, we use the upper bound on $g$. In the second line, we use the fact that the upper bound is symmetric about $r_d$, so integrating over the right half interval is at least as large as the left interval (which is truncated at zero). 
In the fourth line we use the fact that $y\c > 1$ in the interval of integration. (We do this with a change of variables in mind.) In the last line, we apply change of variables. TODO: Actually check the application of COV in the last step. This will be our estimate on $M_{\bar I}$. 

Now, let's come up with a lower bound on the total mass. We'll do this by considering only the mass of $g(r)$ in the subinterval $[r_d-c, r_d]$. For $r$ in this subinterval, we have $f''(r) \leq -2$. This is confirmed by noting that $f''$, as explicitly computed above, is monotonically increasing for $r>0$, and then evaluating $f''$ at the left endpoint of the interval. Applying \eqref{eq:gauss-sphere-f-taylor}, we have 
$$
f(r) \geq f(\sqrt{d-1}) - (r-\sqrt{d-1})^2 \geq f(\sqrt{d-1}) - \frac{c^2}{4}
$$
for $r$ in the designated subinterval. 
Equivalently, this gives
$$
g(r) \geq g(\sqrt{d-1})\exp(- \frac{c^2}{4}). 
$$
Applying this (and noting that the bound has no dependence on $r$), we see that the total mass is at least
$$
\int_{r_d - c}^{r_d} g(r)\dr  \geq cg(\sqrt{d-1})\exp(- \frac{c^2}{4}).
$$
Finally, we see that
 $$
\mbox{QOI} = \frac{M_{\bar I}}{TM} \leq \frac{\frac{2}{c}g(\sqrt{d-1})\exp(-\frac{c^2}{2})}{cg(\sqrt{d-1})\exp(- \frac{c^2}{4})} = \frac{2}{c^2}\exp(- \frac{c^2}{4}).
 $$
 This proves the following lemma (wording directly copied from Blume/Kannan/Hopcroft book). 
 \begin{lemma}
 For a $d$-dimensional spherical Gaussian of variance 1, all but $\frac{2}{c^2}\exp(- \frac{c^2}{4})$ fraction of its mass is within the annulus $\sqrt{d-1} - c \leq r \leq \sqrt{d-1} + c$. 
 \end{lemma}
 Some concrete implications: Independent of $d$, $.99$ fraction of the mass is contained in the annulus with $c = 3.38$. and $.999$ fraction of the mass is contained in the annulus with $c=4.32$. 
 
 \vspace{1em}
\noindent \textbf{Can we do better?} This estimate holds for $d$ arbitrarily large. One may wonder if this is a conservative estimate. Perhaps the mass actually concentrates onto an arbitrarily narrow annulus as $d\to \infty$? My experimentation suggests that this is not the case. I think that the width of the high mass region is genuinely ``constant'' independent of $d$. 
 
 Example: To generate samples from a traditional VAE, one draws samples from a unit gaussian and then passes these through a neural network. For moderately high dimensional latent space, this means that the samples are roughly drawn from the surface of a unit sphere. A question one might ask is: How does this affect interpolation? One of the most useful properties of a VAE is the ability to interpolate in latent space. But, if you draw two samples from a high dimensional gaussian and consider the euclidean ``straight line'' interpolation between them, are you passing through regions where there is very little mass, and so you probably have few samples? Or do your interpolations tend to stay close to the surface of the sphere as well? Is there a better way/different geometry to interpolate rather than straight lines?
 
 Example: TODO: Give an information theory example, like from the Shannon paper about communication in the presence of noise?

TODO: Show some simulation examples. 


%Outline:
%- Question: If you sample from a high dimensional gaussian, can you say roughly where it will lie radially?
%- Idea: the density has the most mass near the origin. But there's almost no volume here. The probability of drawing a sample from a set $A$ is $\int_A f_X(x)\dx$. There's almost no volume here. So the probability of drawing a sample will be small. Eventually, when you get far out, there's tons of volume, but the density drops off. So, again, unlikely to get samples out there. Is there a happy medium where most of the mass lies? (Think of it like, high density in the middle, but no volume, so low overall mass. The probability of sampling from a given region is given by the mass of the region. It's a weird but cool way to think about things.) 
%- Derive density in terms of r
%- Show that most of the mass lies near the surface of a sphere. (TODO: There is one step in this proof that I'm not following. Go back and look this up.) 
%- Show a plot of the density we derived. Show that it clearly concentrates near where we said it would. 
%- Draw some samples from a high dimensional gaussian.  
%- Comments:
%  - we think that samples would lie near the mode. But they don't. Also, a gaussian is log concave. So we sort of think that samples should roughly lie in a convex set. But they really don't. Which is important, I imagine, because if you think a set is convex but it's not, you can get yourself into some real trouble, especially if you're in optimization land. 
%  - Example: Information theory and perturbations of high dimensional signals. 

%\begin{itemize}
%  \item introduce idea of high dimensional Gaussian characterization. Basically, there is a lot more volume on the surface of a sphere in high dimensions. But Gaussian tail is exponential. At some point, drop off in probability with overwhelm increase in volume due to space. So, where is mass most concentrated?
%  \item Review change of variables formula (May need to add some notes just on this. Introduce it. Give some intuition for what it means. Give some examples in probability. (Just illustrate it on a few different distributions.) Give example with law of the unconscious statistician. Give example deriving polar change of coordinates. Give example with reparameterization trick?) 
%  \item Use change of variables to derive distribution of radius r of a high dimensional gaussian (start from chi squared)
%  \item Use this form to derive estimate on where most of the mass lies radially
%  \item Show analytically where the pdf of a high dimensional normal lies. 
%  \item Show in simulation where it lies. 
%  \item Review takeaways: Basically, this helps to build better intuition about how high dimensional random variables/probability works. Also, incidentally, useful in information theory. Also, cite paper about using high dimensional gaussians to characterize loss surface of neural networks. 
%\end{itemize}


%- The mass of a high dimensional  ($d$-dim) gaussian is concentrated on the surface of a sphere of radius $\sqrt{d-1}$. 
%- The question we're wondering about is: If you pick a random draw from a high dimensional gaussian, can you estimate the radius? (Why are we asking this? I'm not certain, actually. It does come up. When? Information theory. Also, do you model high dimensional data as being drawn from a Gaussian? I'm not sure. But I'm certain this comes up a lot. So I should be able to find some compelling motivation later.) 
%- The central thing I want to show is that the mass of a high dimensional gaussian is concentrated about the surface of a sphere. 
%- Having motivated that we want to understand where most of the mass of a high dimensional gaussian lies, and in particular, the length of a random gaussian vector, we can start by building intuition in 1-2 dimensions. Here, most of the mass is clearly concentrated about the origin. A 3d Gaussian is harder to visualize. Intuition suggests that this pattern continues into higher dimensions. But it doesn't. 
%- To see this, show some simulations of a high dimensional Gaussian, and where it concentrates radially. 
%- The reason for this "concentration of mass" in an annulus is that, despite the pdf being large near the center of a sphere, there is a relatively small amount of volume there. But there is tons of volume in larger outer shells. Here, we have the competing factors of the amount of volume increasing exponentially radially outwards (so we have more space to integrate over), and the gaussian distribution decaying squared-exponentially radially. So, there's a happy medium where it settles. 
%- Before considering high dimensional Gaussians, let's consider geometry of high dimensional spheres. Build intuition that if you have a concrete rod, circle, and sphere, more and more of the mass is concentrated in the outer shell of width epsilon. This seems clear if you consider the three objects and then ask which contains the most mass in its outer "shell". This pattern continues into higher dimensions. 
%- Spend some time developing intuition for high dimensional balls. key points - Most of mass of the unit ball is on the surface. Weird. Interior is nearly empty. - Comparison of high dimensional cube vs sphere. - Discussion about most of the mass lying near the equator. 
%- From here, transition back to high dimensional gaussians. Derive the density function. Do this using change of variables from a chi2 distribution. Then, using the density function, derive the amount of mass contained in an annulus using p21 from these notes https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/chap1-high-dim-space.pdf.



Todos
\begin{itemize}
  \item Geometry of high dimensional spheres and cubes
  \item Gaussian in high dimensions (characterize. Maybe there's a reference in one of those old papers about characterizing local minima of neural networks.) 
  \item Law of unconscious statistician (do this and polar and Gaussian constant as applications of change of variables. Then reference all kinds of generative models.) 
  \item Brachistochrone problem
  \item integration by parts and connections with FTC.
  \item review of IFT and connection to inverse function theorem? And connections to FTC?
\end{itemize}

%\section{PCA}
%\begin{itemize}
%  \item Computationally, PCA is just the eigendecomposition of the sample covariance matrix. Review the sample covariance matrix. Why do we use this? (We just do?) 
%  \item Two common ways of looking at this/interpreting the meaning of the eigenvectors/deriving that this eigendecomposition is what you're looking for. 
%  \item 1. Compute a unit vector d such that the projection of data points onto d has maximal variance. 
%  \item 2. You want a lower dimensional subspace, of some fixed dimension $d$ such that the projection of the data onto the subspace has minimal reconstruction error. 
%  \item 3. Say your data is represented as a matrix $X$ (assume centered data). Compute the best low rank approximation to $X$
%  \item Comment: Why are 1 and 2 equivalent? Think of projecting a point onto a subspace. Consider the triangle between the point, the projection, and the origin. The data is fixed. You can change the projection subspace, which implicitly changes the other two legs of the triangle. Minimizing the reconstruction leg is equivalent to maximizing the variance leg. That's one data point. When you sum these together, the same property should hopefully still hold. 
%  \item Walk through each of these. For the first one, you have to set things up by reviewing the KKT conditions. All that's really needed for this is to define the dual function. Show the lower bound property. Then state the KKT conditions for convex problems. Then show that a solution to these conditions implies optimality for a convex problem. (As a sidenote, I think it would be really interesting to work through some problems where the lagrange multipliers have a physical interpretation. Like, force onto a wall. Or an electrical circuit. Or an economics example.) 
%  \item It's relatively easy to show the maximum variance perspective from here. 
%  \item Then show the projection perspective. 
%  \item I think the low rank approximation perspective is really easy. The best low rank approximation is just the SVD. And the definition of the SVD is basically the eigendecomposition of $XX^T$. (Actually, this perspective makes it obvious why you can solve a different system, depending on whether $d$ or $N$ is larger. You just solve for the right or left singular vectors. 
%  \item Then you have kernel PCA. What do I want to know about this? Start by reviewing what we're trying to do here. Send our data into high dimensional feature space. Then figure out the direction of maximal variance (need it exist?!) And then look at the corresponding latent codes as represented via the canonical basis in $R^n$? Hm. That's kind of weird. 
%\end{itemize}
