{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6681a112",
   "metadata": {},
   "source": [
    "In this notebook, we apply MCMC to a simple deciphering task. The problem is interesting, because the state space of the markov chain is permutations of the alphabet, so there are (26 + 1)! $\\approx 10^{28}$ states. The main point of this is to show how MCMC can easily sample from a bizarre distribution defined over a huge state space. The MCMC algorithm itself is quite simple and converges surprisingly quickly. \n",
    "\n",
    "The goal is to crack a substitution cipher. That is, a cipher where every letter of the alphabet has been switched for some symbol (we'll just use the letter of the alphabet again as our symbols). The way we intend to solve this is to set up a Markov chain where the stationary distribution is concentrated, in some sense, on the \"most plausible\" deciphering schemes. We'll do this as follows.  \n",
    "\n",
    "Firsts, compute a bigram model of English using some source text that you think statistically resembles the coded text. We'll use war and peace. Let $b(x, y)$ denote the frequency of the bigram $(x,y)$ in your source text. Let $f$ be a deciphering scheme (mapping from coded symbols to english letters). We'll define the plausiblity of $f$ to be\n",
    "$$\n",
    "Pl(f) := \\prod_{c_i} b(f(c_i), f(c_{i+1})),\n",
    "$$\n",
    "where $c_i$ denotes the $i$th character in your coded message. Basically, if you decode your message with $f$ and it gives a bunch of unlikely bigrams (meaning they rarely occur in your reference text), then it's implausible. On the other hand, if many of the bigrams that appear in the decoded message are also common in your reference text, then you consider it to be more plausible. \n",
    "\n",
    "Now, observe that the set of possible $f$'s is essentially the set of permutations of the alphabet. (e.g., if we're going from regular alphabet symbols to regular alphabet symbols, then then your cipher is just a permutation map that tells you which letter in a regular message maps to which coded letter). We'll use lowercase letters and include a space character, so we have $27!$ states. Let\n",
    "$$\n",
    "\\pi(f; b, \\text{message}) := \\frac{Pl(f)}{\\sum_g Pl(g)}\n",
    "$$\n",
    "denote a normalization of $Pl(f)$, where the sum is over all possible permutations $g$. I added the parameters $b$ and the coded message as \"parameters\" to $\\pi$ to emphasize that distribution depends critically on both of these. If you change your bigram model or your decoded message, then you're going to shift $\\pi$. \n",
    "\n",
    "We want to sample from $\\pi$. The idea is that most of the mass of $\\pi$ should be on reasonable decoding schemes. So, a sample from it should give a decent first guess at a decoding scheme. \n",
    "\n",
    "To sample from $\\pi$ we'll use a very simple MCMC algorithm that will be described below. The algorithm is really strikingly simple. \n",
    "\n",
    "We'll start by building a bigram model. We'll use a copy of war and peace downloaded from project gutenberg as a reference text. Nothing too interesting here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9111c107",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[1;32m     15\u001b[0m     chs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(w) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ch1, ch2, ch3 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(chs, chs[\u001b[38;5;241m1\u001b[39m:], ch[\u001b[38;5;241m2\u001b[39m:]):\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m         \u001b[38;5;66;03m# Get rid of weird characters\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m ch1 \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m alphabet \u001b[38;5;129;01mor\u001b[39;00m ch2 \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m alphabet \u001b[38;5;129;01mor\u001b[39;00m ch3 \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m alphabet:\n\u001b[1;32m     20\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ch' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "recompute_bigram = True\n",
    "smoothing_k = 1  # smoothing factor for bigram model (pretend there are this many examples to start with for each bigram)\n",
    "if recompute_bigram:\n",
    "    # read in war and peace\n",
    "    words = open('data/war_and_peace.txt', 'r').read().lower().split()\n",
    "\n",
    "    # compute bigram model\n",
    "    b = {}\n",
    "    alphabet = set(' .!abcdefghijklmnopqrstuvwxyz')  # restrict alphabet to 26 + 1 characters\n",
    "    for w in words:\n",
    "        chs = [' '] + list(w) + [' ']\n",
    "        for ch1, ch2, ch3 in zip(chs, chs[1:], ch[2:]):\n",
    "\n",
    "            # Get rid of weird characters\n",
    "            if ch1 not in alphabet or ch2 not in alphabet or ch3 not in alphabet:\n",
    "                continue\n",
    "\n",
    "            # track frequency\n",
    "            bigram = (ch1, ch2, ch3)\n",
    "            b[bigram] = b.get(bigram, 0) + 1\n",
    "\n",
    "    # normalize bigram model\n",
    "    total = sum((v for v in b.values())) + smoothing_k*len(alphabet)  # + stuff for smoothing\n",
    "    for k, v in b.items():\n",
    "        b[k] = v/total\n",
    "\n",
    "    stuff = (alphabet, b)\n",
    "    pickle.dump(stuff, open('bigram_model.p', 'wb'))\n",
    "\n",
    "else:\n",
    "    alphabet, b = pickle.load(open('bigram_model.p', 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675482d0",
   "metadata": {},
   "source": [
    "Next, let's pick a message we want to code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b91b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text =  \"hello brian! i am glad you figured out the code. with a little help from your computer of course. i had to think for a second to make this longer but this is it. hopefully the computer can guess it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f8d888",
   "metadata": {},
   "source": [
    "And now let's generate a random cipher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162ab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # come up with a cipher\n",
    "sorted_alphabet = sorted(alphabet)\n",
    "# sigma = np.random.permutation(sorted_alphabet)\n",
    "# print(sigma)\n",
    "# cipher = {}\n",
    "# for i, ch in enumerate(sorted_alphabet):\n",
    "#     cipher[ch] = sigma[i]\n",
    "\n",
    "cipher = {'a': \".\",\n",
    "          'b': \"f\",\n",
    "          'c': \"z\",\n",
    "          'd': \"x\",\n",
    "          'e': \"!\",\n",
    "          'f': \"q\",\n",
    "          'g': \"b\",\n",
    "          'h': \"g\",\n",
    "          'i': \" \",\n",
    "          'j': \"y\",\n",
    "          'k': \"w\",\n",
    "          'l': \"o\",\n",
    "          'm': \"i\",\n",
    "          'n': \"c\",\n",
    "          'o': \"a\",\n",
    "          'p': \"p\",\n",
    "          'q': \"t\",\n",
    "          'r': \"h\",\n",
    "          's': \"d\",\n",
    "          't': \"s\",\n",
    "          'u': \"l\",\n",
    "          'v': \"m\",\n",
    "          'w': \"n\",\n",
    "          'x': \"u\",\n",
    "          'y': \"v\",\n",
    "          'z': \"j\",\n",
    "          ' ': \"e\",\n",
    "          '!': \"r\",\n",
    "          '.': \"k\",\n",
    "          }\n",
    "\n",
    "print(f'cipher={cipher}')\n",
    "    \n",
    "# compute inverse cipher. Mostly for debugging...\n",
    "real_inv_cipher = {}\n",
    "for k, v in cipher.items():\n",
    "    real_inv_cipher[v] = k\n",
    "\n",
    "# cipher some input text\n",
    "cipher_text = ''.join([cipher[ch] for ch in input_text])\n",
    "print(f'ciphertext = {cipher_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f5c29e",
   "metadata": {},
   "source": [
    "Before running the MCMC algorithm, here are a couple of simple helper functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9466a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decipher_text(cipher_candidate):\n",
    "    \"Using the cipher candidate, decode the cipher_text above.\"\n",
    "    deciphered_text = []\n",
    "    for ch in cipher_text:\n",
    "        deciphered_text.append(cipher_candidate[ch])\n",
    "    return deciphered_text\n",
    "\n",
    "def plaus(cipher_candidate):\n",
    "    \"Compute plausiblity, Pl(cipher_candidate).\"\n",
    "    deciphered_text = decipher_text(cipher_candidate)\n",
    "    score = 0\n",
    "    for ch1, ch2 in zip(deciphered_text, deciphered_text[1:]):\n",
    "        score += np.log(b.get((ch1, ch2), smoothing_k/total))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1110d2cc",
   "metadata": {},
   "source": [
    "As a sanity check, let's apply the real decipher key and make sure it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c6ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check that the real decipher key works\n",
    "deciphered_text = \"\".join(decipher_text(real_inv_cipher))\n",
    "print(f'deciphered text: {deciphered_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12543f9",
   "metadata": {},
   "source": [
    "Now we're ready for MCMC. \n",
    "\n",
    "Pseudo code for the algorithm is as follows (taken from Diaconis):\n",
    "1. Start with a random permutation $f$.\n",
    "2. compute $Pl(f)$.\n",
    "3. Change to $f^*$ by making a random transposition of the values $f$ assigns to two symbols.\n",
    "4. Compute $Pl(f^*)$; if this is larger than $Pl(f)$, accept $f^*$.\n",
    "5. If not, flip a $Pl(f^*)/Pl(f)$ coin; if it comes up heads, accept $f^*$.\n",
    "6. If the coin comes up tails, stay at $f$.\n",
    "\n",
    "This is an instance of the metropolis algorithm. Which is a special subclass of metropolis hasings algorithms where the proposal distribution is symmetric. This is why the accept rule only has the target distribution in it (the usual proposal distribution terms from metropolis hasings cancel out due to symmetry.) \n",
    "\n",
    "The metropolis algorithm is discussed in Algorithm A.29, p.288 in MCSM. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8aabf0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'length of ciphertext={len(cipher_text)}')\n",
    "# for later reference, print out the plausiblity score of the real deciphering scheme\n",
    "print(f'REAL CIPHER PLAUSIBLITY: {plaus(real_inv_cipher)}')\n",
    "\n",
    "# initialize random (inverse) cipher guess\n",
    "sigma = np.random.permutation(sorted_alphabet)\n",
    "inv_cipher = {}\n",
    "for i, ch in enumerate(sorted_alphabet):\n",
    "    inv_cipher[ch] = sigma[i]\n",
    "\n",
    "# Run metropolis algorithm\n",
    "n_steps = 20_000\n",
    "for t in range(n_steps):\n",
    "    # Generate permutation candidate (this should be a uniform random sample from the set of permutations, right?)\n",
    "    new_cipher = inv_cipher.copy()\n",
    "    ch1, ch2 = random.sample(sorted_alphabet, 2)\n",
    "    temp_val = new_cipher[ch1]\n",
    "    new_cipher[ch1] = new_cipher[ch2]\n",
    "    new_cipher[ch2] = temp_val\n",
    "\n",
    "    # metropolis update\n",
    "    if plaus(new_cipher) > plaus(inv_cipher):\n",
    "        inv_cipher = new_cipher\n",
    "    else:\n",
    "        threshold = np.exp(plaus(new_cipher) - plaus(inv_cipher))\n",
    "        if np.random.uniform() < threshold:\n",
    "            inv_cipher = new_cipher\n",
    "\n",
    "    if t % 100 == 0:\n",
    "        deciphered_text = \"\".join(decipher_text(inv_cipher))[:80]\n",
    "        print(f't={t}; score = {plaus(inv_cipher):.2f}; text={deciphered_text}...')\n",
    "        #print(f'score = {plaus(inv_cipher)}, real cipher score = {plaus(real_inv_cipher)}', )\n",
    "#         print(f'{deciphered_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47286554",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_cipher[' '] = 'i'\n",
    "inv_cipher['.'] = 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a04acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inv_cipher)\n",
    "print(cipher_text)\n",
    "deciphered_text = \"\".join(decipher_text(inv_cipher))\n",
    "print(f't={t}; score = {plaus(inv_cipher):.2f}; text={deciphered_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a60d24b",
   "metadata": {},
   "source": [
    "Note: Score is $ln(Pl(f))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96322644",
   "metadata": {},
   "source": [
    "Closing thoughts:\n",
    "\n",
    "\n",
    "- The point of this example isn't that this is a useful decryption scheme. It's that we've been able to sample from this really weird and custom distribution. We've defined a distribution over a state space of $~10^{28}$ permutations. The exact distribution we need to sample from changes whenever we change the message or the bigram reference. Yet, we seem to get reasonable mixing within a few thousand iterations without any special initialization.\n",
    "- It is sensitive to initialization. Some initializations don't work well at all. But you don't to try too hard. A uniform random one will get you there without too much work. \n",
    "- The choice of proposal distribution is critical. If you mess with the way new permutations are proposed or the accept threshold, it can easily go haywire. \n",
    "- The fact that we're trying to construct a distribution that concentrates on the set of maxima of some function begs the idea that we consider simulated annealing. This is accomplished with a really simple modification of this code. Just add a temperature parameter to the cost function in the form of an exponent. As the exponent goes $\\infty$ (so, use $1/T$ as the exponent, where $T$ is the temperature), $\\pi$ concentrates on the set of maxima of $Pl(f)$. \n",
    "\n",
    "\n",
    "Some useful references:\n",
    "- \"The markov chain monte carlo revolution\", Perci Diaconis\n",
    "- \"Decrypting classical cipher text using Markov chain Monte Carlo\", Jian Chen, Jeffrey S. Rosenthal\n",
    "- (MCSM) Monte Carlo statistical methods, book by Roberts, and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c857bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
